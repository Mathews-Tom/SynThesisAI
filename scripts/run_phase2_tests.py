#!/usr/bin/env python3
"""
Script to run all Phase 2 MARL coordination tests.

This script runs all unit tests and integration tests for Phase 2 of the MARL coordination system.
"""

# Standard Library
import logging
import subprocess
import sys
from pathlib import Path
from typing import Dict, List
from subprocess import CompletedProcess


# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


def run_tests() -> int:
    """Run all Phase 2 tests.

    Returns:
        int: 0 if all tests pass, 1 if any tests fail.
    """
    logger.info("ğŸš€ Running Phase 2 MARL coordination tests...")

    # Define test categories and their corresponding test files
    test_categories: Dict[str, List[str]] = {
        "MARL Agents": [
            "tests/unit_tests/test_base_agent.py",
            "tests/unit_tests/test_generator_agent.py",
            "tests/unit_tests/test_validator_agent.py",
            "tests/unit_tests/test_curriculum_agent.py",
        ],
        "Coordination Mechanisms": [
            "tests/unit_tests/test_coordination_policy.py",
            "tests/unit_tests/test_consensus_manager.py",
            "tests/unit_tests/test_communication_protocol.py",
        ],
        "Shared Learning": [
            "tests/unit_tests/test_shared_experience.py",
            "tests/unit_tests/test_continuous_learning.py",
        ],
        "Performance Monitoring": [
            "tests/unit_tests/test_performance_monitor.py",
            "tests/unit_tests/test_performance_analyzer.py",
            "tests/unit_tests/test_performance_reporter.py",
        ],
        "Configuration Management": [
            "tests/unit_tests/test_config_manager.py",
            "tests/unit_tests/test_config_validator.py",
        ],
        "Experimentation Framework": [
            "tests/unit_tests/test_experiment_manager.py",
            "tests/unit_tests/test_ab_testing.py",
        ],
        "Error Handling": [
            "tests/unit_tests/test_error_handling.py",
        ],
        "Fault Tolerance": [
            "tests/unit_tests/test_fault_tolerance.py",
        ],
    }

    # Track overall results
    all_results: Dict[str, CompletedProcess] = {}
    overall_success = True

    # Run tests by category
    for category, test_files in test_categories.items():
        logger.info("\nğŸ“‹ Running %s tests...", category)

        # Filter existing test files
        existing_files = []
        for test_file in test_files:
            if Path(test_file).exists():
                existing_files.append(test_file)
            else:
                logger.warning("âš ï¸  Test file not found: %s", test_file)

        if not existing_files:
            logger.warning("âš ï¸  No test files found for %s", category)
            continue

        # Run tests for this category
        result = subprocess.run(
            ["uv", "run", "python", "-m", "pytest"] + existing_files + ["-v", "--tb=short"],
            capture_output=True,
            text=True,
            check=False,
        )

        all_results[category] = result

        # Print results
        if result.returncode == 0:
            logger.info("âœ… %s tests PASSED", category)
        else:
            logger.error("âŒ %s tests FAILED", category)
            overall_success = False

        # Print test output (abbreviated)
        if result.stdout:
            lines = result.stdout.split("\n")
            # Show summary line
            for line in lines:
                if "passed" in line and "failed" in line:
                    logger.info("   %s", line.strip())
                    break
                elif "passed" in line and line.strip().endswith("passed"):
                    logger.info("   %s", line.strip())
                    break

        if result.stderr and result.returncode != 0:
            logger.error("   Error output: %s...", result.stderr[:200])

    # Run end-to-end tests
    logger.info("\nğŸ“‹ Running End-to-End tests...")
    e2e_result = subprocess.run(
        [
            "uv",
            "run",
            "python",
            "-m",
            "pytest",
            "tests/end_to_end/test_phase2_comprehensive.py",
            "-v",
            "--tb=short",
        ],
        capture_output=True,
        text=True,
        check=False,
    )

    all_results["End-to-End"] = e2e_result

    if e2e_result.returncode == 0:
        logger.info("âœ… End-to-End tests PASSED")
    else:
        logger.error("âŒ End-to-End tests FAILED")
        overall_success = False
        if e2e_result.stderr:
            logger.error("   Error output: %s...", e2e_result.stderr[:200])

    # Print comprehensive summary
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š PHASE 2 TEST SUMMARY")
    logger.info("=" * 60)

    total_categories = len(all_results)
    passed_categories = sum(1 for result in all_results.values() if result.returncode == 0)

    for category, result in all_results.items():
        status = "âœ… PASSED" if result.returncode == 0 else "âŒ FAILED"
        logger.info("%s", f"{category:.<30} {status}")

    logger.info("-" * 60)
    logger.info("Categories passed: %d/%d", passed_categories, total_categories)

    if overall_success:
        logger.info("ğŸ‰ ALL PHASE 2 TESTS PASSED! MARL coordination system is ready!")
        logger.info("âœ¨ Key achievements:")
        logger.info("   â€¢ Multi-agent RL coordination implemented")
        logger.info("   â€¢ Consensus mechanisms working")
        logger.info("   â€¢ Shared learning infrastructure operational")
        logger.info("   â€¢ Performance monitoring active")
        logger.info("   â€¢ Configuration management ready")
        logger.info("   â€¢ Experimentation framework functional")
        logger.info("   â€¢ Error handling and fault tolerance robust")
        return 0
    else:
        failed_categories = total_categories - passed_categories
        logger.error(
            "âš ï¸  %d test categories failed. Please review and fix.",
            failed_categories,
        )
        logger.error("ğŸ”§ Common issues to check:")
        logger.error("   â€¢ Missing dependencies or imports")
        logger.error("   â€¢ Configuration file issues")
        logger.error("   â€¢ Async/await syntax problems")
        logger.error("   â€¢ Mock object setup")
        return 1


def print_detailed_results() -> None:
    """Print detailed test results for debugging.

    Returns:
        None
    """
    logger.info("\nğŸ” For detailed test output, run individual test categories:")
    logger.info("   uv run pytest tests/unit_tests/test_error_handling.py -v")
    logger.info("   uv run pytest tests/unit_tests/test_fault_tolerance.py -v")
    logger.info("   uv run pytest tests/unit_tests/test_experiment_manager.py -v")
    logger.info("   uv run pytest tests/end_to_end/test_phase2_comprehensive.py -v")


if __name__ == "__main__":
    exit_code = run_tests()
    if exit_code != 0:
        print_detailed_results()
    sys.exit(exit_code)
